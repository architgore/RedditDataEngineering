#  Data Pipeline with Reddit, Airflow, Celery, Postgres, S3, AWS Glue, Athena, and Redshift
# This project delivers a detailed data pipeline system for extracting, transforming, and loading Reddit data into a Redshift database. It utilizes various tools such as Apache Airflow, Celery, PostgreSQL, Amazon S3, AWS Glue, Amazon Athena, and Amazon Redshift.

# Table of Contents
Overview
Architecture
Prerequisites
System Setup
Video
Overview
The purpose of this pipeline is to:

Extract Reddit data using its API.
Deposit the raw data into an S3 bucket via Airflow.
Process the data using AWS Glue and Amazon Athena.
Import the processed data into Amazon Redshift for analysis and querying.
Architecture
RedditDataEngineering.png

Reddit API: Initial data source.
Apache Airflow & Celery: Coordinates and manages the ETL tasks.
PostgreSQL: Provides temporary storage and manages metadata.
Amazon S3: Stores the raw data.
AWS Glue: Handles data cataloging and ETL operations.
Amazon Athena: Executes SQL-based transformations.
Amazon Redshift: Used for data storage and analytics.
Prerequisites
An AWS account with the necessary permissions for S3, Glue, Athena, and Redshift.
Credentials for the Reddit API.
Docker installed.
Python version 3.9 or later.
System Setup
Download the repository.
bash
Copy code
 git clone https://github.com/airscholar/RedditDataEngineering.git
Set up a virtual environment.
bash
Copy code
 python3 -m venv venv
Activate the virtual environment.
bash
Copy code
 source venv/bin/activate
Install required packages.
bash
Copy code
 pip install -r requirements.txt
Update the configuration and credentials files.
bash
Copy code
 mv config/config.conf.example config/config.conf
Initialize the Docker containers.
bash
Copy code
 docker-compose up -d
Access the Airflow web interface.
bash
Copy code
 open http://localhost:8080

